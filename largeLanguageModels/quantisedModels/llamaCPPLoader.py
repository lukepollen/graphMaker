### Imports ###

from llama_cpp import Llama

### End of Imports ###


### Function Definition ###



### End of Function Definition ###

llm = Llama(
      model_path=r"../../Meta-Llama-3-8B-Q4_5_M.gguf",
      #n_gpu_layers=-1, # Uncomment to use GPU acceleration
      #seed=1337, # Uncomment to set a specific seed
      n_ctx=2048, # Uncomment to increase the context window
)

# Generate a response back from the large language model we have loaded, using our parameters.
output = llm(
    # Prompt to respond to with a series of tokens generated by the LLM
    """
    How big is the moon?
    """,

    echo=True, # Do not or not do the print out prompt in the response from the LLM (there is no try).
    frequency_penalty=1,
    max_tokens=1024, #Generate up to the this number of tokens. Using None generates to the end of the context window
    min_p=0.02,
    repeat_penalty=1.5,
    temperature=0.7,
    #top_k = 0.9,
    #top_p = 0.9,
)
print(output.get('choices')[0].get('text'))

### Errata ###

"""
prompt: The prompt to generate text from.
 |          suffix: A suffix to append to the generated text. If None, no suffix is appended.
 |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0 or None, the maximum number of tokens to generate is unlimited and depends on n_ctx.
 |          temperature: The temperature to use for sampling.
 |          top_p: The top-p value to use for nucleus sampling. Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
 |          min_p: The min-p value to use for minimum p sampling. Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841
 |          typical_p: The typical-p value to use for sampling. Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
 |          logprobs: The number of logprobs to return. If None, no logprobs are returned.
 |          echo: Whether to echo the prompt.
 |          stop: A list of strings to stop generation when encountered.
 |          frequency_penalty: The penalty to apply to tokens based on their frequency in the prompt.
 |          presence_penalty: The penalty to apply to tokens based on their presence in the prompt.
 |          repeat_penalty: The penalty to apply to repeated tokens.
 |          top_k: The top-k value to use for sampling. Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
 |          stream: Whether to stream the results.
 |          seed: The seed to use for sampling.
 |          tfs_z: The tail-free sampling parameter. Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.
 |          mirostat_mode: The mirostat sampling mode.
 |          mirostat_tau: The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
 |          mirostat_eta: The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
 |          model: The name to use for the model in the completion object.
 |          stopping_criteria: A list of stopping criteria to use.
 |          logits_processor: A list of logits processors to use.
 |          grammar: A grammar to use for constrained sampling.
 |          logit_bias: A logit bias to use.
"""

### End of Errata  ###

if __name__ == '__main__':

    thePrompt = """
    How big is the moon? Describe the parameters relative to the solar system and the mass of the sun.  
    """
    sauce = [0.6, 0.7]
    repeatPenalties = [0.5, 1.0, 1.5, 2.0, 4.0, 8.0, 16.0]

    for spicyness in sauce:
        for repeatPenalty in repeatPenalties:
            output = llm(
                thePrompt,
                echo=True,
                max_tokens=2048,
                min_p=0.02,
                repeat_penalty=repeatPenalty,
                temperature=spicyness,
            )
            print(output.get('choices')[0].get('text'))